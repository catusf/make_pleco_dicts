{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all word lists to find out the list of important words to use in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(top_words_50k_set)=50000\n",
      "len(thienchuu_set)=9897\n",
      "len(extracted_words_set)=78157\n",
      "len(thienchuu_set)=9897\n",
      "len(current_files_set)=78226\n",
      "len(hsk_1_9_words_set)=10943\n",
      "len(opensub_most_pop_50k_set)=50000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from pinyin import get as pinyinget\n",
    "\n",
    "from tools_configs import *\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from tools_configs import *\n",
    "\n",
    "# Utilities functions\n",
    "\n",
    "NEED_CONVERT = r\"([^\t↑ ,; 0-9a-zA-Z()一-龥])\"\n",
    "\n",
    "list_1m_words = load_frequent_words(TOP_WORDS_1M)\n",
    "top_words_1m_set = frozenset(list_1m_words)\n",
    "top_words_100k_set = frozenset(list_1m_words[:100000])\n",
    "top_words_50k_set = frozenset(list_1m_words[:50000])\n",
    "print(f\"{len(top_words_50k_set)=}\")\n",
    "\n",
    "top_words_100k_index = {\n",
    "    k: v for v, k in enumerate(list_1m_words[:100000])\n",
    "}  # Only care about first 100000 words\n",
    "\n",
    "top_words_24k_index = {k: v for v, k in enumerate(\n",
    "    load_frequent_words(TOP_WORDS_24K))}\n",
    "\n",
    "top_words_50k_index = {k: v for v, k in enumerate(list_1m_words[:50000])}\n",
    "\n",
    "top_words_24k = frozenset(load_frequent_words(TOP_WORDS_24K))\n",
    "\n",
    "thienchuu_set = frozenset(load_frequent_words('thienchuu.txt'))\n",
    "print(f\"{len(thienchuu_set)=}\")\n",
    "\n",
    "# print(f\"CHECK {('蜿' in thienchuu_set)=}\")\n",
    "\n",
    "list_hsk_1_4_list = load_frequent_words('hsk_1-4.txt')\n",
    "list_hsk_1_4_dict = [{x: top_words_24k_index[x]} for x in list_hsk_1_4_list if x in top_words_24k_index]\n",
    "\n",
    "current_word_list = top_words_24k\n",
    "extracted_words_hanzii_popularity = []\n",
    "with open('dict_data.json', 'r', encoding='utf8') as fread:\n",
    "    extracted_data = json.load(fread)\n",
    "    extracted_words_set = frozenset(extracted_data.keys())\n",
    "    print(f'{len(extracted_words_set)=}')\n",
    "\n",
    "    for key in extracted_data:\n",
    "        pop = None if 'hanzii_popularity' not in extracted_data[key] else extracted_data[key]['hanzii_popularity']\n",
    "        extracted_words_hanzii_popularity.append(pop)\n",
    "        \n",
    "print(f\"{len(thienchuu_set)=}\")\n",
    "\n",
    "files = glob.glob(f\"{HTML_FOLDER}/*.html\")\n",
    "\n",
    "current_files_set = set()\n",
    "for num, filepath in enumerate(files):\n",
    "    dict_item = None\n",
    "    # if filepath != 'html\\七大.html':\n",
    "    #     continue\n",
    "\n",
    "    pleco_string = \"\"\n",
    "\n",
    "    # The above code is not doing anything. It only contains the word \"headword\" and three hash\n",
    "    # symbols, which are used to create comments in Python code.\n",
    "    headword, ext = os.path.splitext(os.path.split(filepath)[1])\n",
    "\n",
    "    current_files_set.add(headword)\n",
    "\n",
    "print(f'{len(current_files_set)=}')\n",
    "\n",
    "lcmc_words_set = thienchuu_set = frozenset(load_frequent_words('lcmc_freq.txt'))\n",
    "\n",
    "hsk_1_9_words_set = thienchuu_set = frozenset(load_frequent_words('hsk30-pleco.txt'))\n",
    "\n",
    "count = 0\n",
    "opensub_most_pop_100k_list = list()\n",
    "list_simp = load_frequent_words('opensubtitles_zh_cn_full.txt')\n",
    "\n",
    "while len(opensub_most_pop_100k_list) < 100000:\n",
    "    word = list_simp[count]\n",
    "    if hanzidentifier.is_simplified(word):\n",
    "        opensub_most_pop_100k_list.append(word)\n",
    "    else:\n",
    "        # print(word)\n",
    "        pass\n",
    "\n",
    "    count += 1        \n",
    "\n",
    "opensub_most_pop_50k_set = frozenset(opensub_most_pop_100k_list[:50000])\n",
    "opensub_most_pop_100k_set = frozenset(opensub_most_pop_100k_list[:100000])\n",
    "\n",
    "print(f'{len(hsk_1_9_words_set)=}')\n",
    "print(f'{len(opensub_most_pop_50k_set)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(list_hsk_1_4_dict)=4292\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(list_hsk_1_4_dict)=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(thienchuu_set)=10943\n",
      "len(thienchuu_set)=9897\n",
      "len(add_thienchuu)=2772\n",
      "len(add_top_words_50k)=1910\n",
      "len(add_opensub_50k)=16\n",
      "len(add_hsk)=3\n",
      "len(add_all)=1\n",
      "len(this_dic_words_set)=89684\n",
      "len(words_to_add_set)=11458\n",
      "len(redundant_words)=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_files_with_definition_set = current_files_set\n",
    "print(f'{len(thienchuu_set)=}')\n",
    "thienchuu_set = frozenset(load_frequent_words('thienchuu.txt'))\n",
    "print(f'{len(thienchuu_set)=}')\n",
    "add_thienchuu = thienchuu_set-current_files_with_definition_set\n",
    "add_top_words_50k = top_words_50k_set-current_files_with_definition_set\n",
    "add_opensub_50k = opensub_most_pop_50k_set - current_files_with_definition_set\n",
    "add_hsk = hsk_1_9_words_set - current_files_with_definition_set\n",
    "add_all_raw = add_thienchuu | add_top_words_50k | add_opensub_50k | add_hsk\n",
    "\n",
    "def all_simplified(text):\n",
    "    for i in text:\n",
    "        if not hanzidentifier.is_simplified(i):\n",
    "            return False\n",
    "            break\n",
    "    \n",
    "    return True\n",
    "\n",
    "no_definitions_files = set()\n",
    "add_all = set([word for word in add_all_raw if (all_simplified(word) and word not in no_definitions_files)])\n",
    "\n",
    "print(f'{len(add_thienchuu)=}')\n",
    "print(f'{len(add_top_words_50k)=}')\n",
    "print(f'{len(add_opensub_50k)=}')\n",
    "print(f'{len(add_hsk)=}')\n",
    "\n",
    "print(f'{len(add_all)=}')\n",
    "\n",
    "len(add_all | current_files_with_definition_set)\n",
    "\n",
    "this_dic_words_set_raw_no_lcmc = thienchuu_set | top_words_24k | top_words_50k_set | opensub_most_pop_50k_set | hsk_1_9_words_set\n",
    "\n",
    "this_dic_words_set_raw = thienchuu_set | top_words_24k | top_words_50k_set | opensub_most_pop_50k_set | hsk_1_9_words_set | lcmc_words_set\n",
    "\n",
    "this_dic_words_set = frozenset([word for word in this_dic_words_set_raw if (all_simplified(word) and word not in no_definitions_files)])\n",
    "\n",
    "words_to_add_set = this_dic_words_set - current_files_set\n",
    "print(f'{len(this_dic_words_set)=}')\n",
    "print(f'{len(words_to_add_set)=}')\n",
    "\n",
    "redundant_words = current_files_set - this_dic_words_set\n",
    "print(f'{len(redundant_words)=}')\n",
    "redundant_words - thienchuu_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import os\n",
    "import shutil\n",
    "# with open('2023-12-07_17-38-00-error.log', \"r\", encoding=\"utf-8\") as fin:\n",
    "#     # Remove first line with comments\n",
    "#     no_definitions_files = frozenset([re.split(r'[.\\\\]', line)[1] for line in fin.readlines()])\n",
    "#     print(f'{len(no_definitions_files)=}')\n",
    "\n",
    "#     with open('redownload.txt', \"w\", encoding=\"utf-8\") as fread:\n",
    "#         fread.writelines([line+'\\n' for line in no_definitions_files])\n",
    "        \n",
    "#     for filename in no_definitions_files:\n",
    "#         if os.path.exists(f'html/{filename}.html'):\n",
    "#             shutil.move(f'html/{filename}.html', f'html/bad/{filename}.html', )\n",
    "\n",
    "# with open('redownload.txt', \"w\", encoding=\"utf-8\") as fread:\n",
    "#     fread.writelines([line+'\\n' for line in words_to_add_set])\n",
    "\n",
    "with open('dic_words_set.txt', \"w\", encoding=\"utf-8\") as fread:\n",
    "    fread.writelines([line+'\\n' for line in sorted(list(this_dic_words_set))])\n",
    "\n",
    "# for filename in redundant_words:\n",
    "#     if os.path.exists(f'html/{filename}.html'):\n",
    "#         shutil.move(f'html/{filename}.html', f'html/redundant/{filename}.html', )\n",
    "\n",
    "# with open('new_urls.txt', \"r\", encoding=\"utf-8\") as fin:\n",
    "#     # Remove first line with comments\n",
    "#     all_words_from_html = set([url_to_headword(line.strip()) for line in fin.readlines()])\n",
    "#     print(f'{len(all_words_from_html)=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(top_words_24k_characters)=3384\n",
      "len(top_words_100k_characters)=8836\n",
      "len(top_words_1m_characters)=16662\n"
     ]
    }
   ],
   "source": [
    "top_words_24k_characters = set()\n",
    "for word in top_words_24k:\n",
    "#    print(word)\n",
    "    top_words_24k_characters.update(list(word))\n",
    "\n",
    "print(f'{len(top_words_24k_characters)=}')\n",
    "\n",
    "top_words_100k_characters = set()\n",
    "for word in top_words_100k_set:\n",
    "#    print(word)\n",
    "    top_words_100k_characters.update(list(word))\n",
    "\n",
    "print(f'{len(top_words_100k_characters)=}')\n",
    "\n",
    "top_words_1m_characters = set()\n",
    "for word in top_words_1m_set:\n",
    "#    print(word)\n",
    "    top_words_1m_characters.update(list(word))\n",
    "\n",
    "print(f'{len(top_words_1m_characters)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CHECK {'蜿' in thienchuu_set}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thienchuu count: 9897\n",
      "Current files: 78226\n",
      "Current files with definition: 78226\n",
      "In both Thienchuu and Current_files_set: 7125\n",
      "In Thienchuu but not in current_files_set: 2772\n",
      "In Thienchuu but not in top_words_24k: 7451\n",
      "In Thienchuu but not in top_words_100k: 3497\n",
      "Length of words and frequency in current_files_set [(2, 46170), (3, 15427), (1, 7802), (4, 7755), (5, 678), (6, 214), (7, 106), (8, 33), (9, 22), (11, 10), (10, 8), (13, 1)]\n",
      "len(top_words_24k_characters)=3384\n",
      "len(top_words_100k_characters)=8836\n",
      "len(top_words_1m_characters)=16662\n",
      "Length of words and frequency in top_words_24k [(2, 15507), (3, 4236), (1, 2553), (4, 1749), (5, 364), (6, 113), (7, 84), (8, 29), (9, 20), (11, 7), (10, 6), (13, 1)]\n",
      "Length of words and frequency in top_words_100k_set [(2, 52523), (3, 24365), (4, 11741), (1, 8571), (5, 1562), (6, 688), (7, 294), (8, 105), (9, 55), (10, 49), (11, 22), (12, 12), (13, 7), (14, 3), (15, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Thienchuu count: {len(thienchuu_set)}')\n",
    "print(f'Current files: {len(current_files_set)}')\n",
    "# print(f'all_words_from_html: {len(all_words_from_html)}')\n",
    "# print(f'no_definitions_files: {len(no_definitions_files)}')\n",
    "# current_files_with_definition_set = current_files_set.difference(no_definitions_files)\n",
    "print(f'Current files with definition: {len(current_files_with_definition_set)}')\n",
    "\n",
    "print(f'In both Thienchuu and Current_files_set: {len(current_files_with_definition_set.intersection(thienchuu_set))}')\n",
    "print(f'In Thienchuu but not in current_files_set: {len(thienchuu_set.difference(current_files_with_definition_set))}')\n",
    "print(f'In Thienchuu but not in top_words_24k: {len(thienchuu_set.difference(top_words_24k))}')\n",
    "print(f'In Thienchuu but not in top_words_100k: {len(thienchuu_set.difference(top_words_100k_set))}')\n",
    "# print(f'In all_words_from_html but not in top_words_100k: {len(all_words_from_html.difference(top_words_100k_set))}')\n",
    "# print(f'In both current_files_set AND all_words_from_html: {len(current_files_with_definition_set.intersection(all_words_from_html))}')\n",
    "print(f'Length of words and frequency in current_files_set {Counter([len(file) for file in current_files_with_definition_set]).most_common(100)}')\n",
    "\n",
    "print(f'{len(top_words_24k_characters)=}')\n",
    "print(f'{len(top_words_100k_characters)=}')\n",
    "print(f'{len(top_words_1m_characters)=}')\n",
    "\n",
    "# print(f'Length of words and frequency in all_words_from_html {Counter([len(file) for file in all_words_from_html]).most_common(100)}')\n",
    "print(f'Length of words and frequency in top_words_24k {Counter([len(file) for file in top_words_24k]).most_common(100)}')\n",
    "print(f'Length of words and frequency in top_words_100k_set {Counter([len(file) for file in top_words_100k_set]).most_common(100)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(extracted_words_set)=78157\n",
      "len(top_words_24k)=24669\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(extracted_words_set)=}')\n",
    "print(f'{len(top_words_24k)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('hanzii_pop.txt', 'w', encoding='utf-8') as fwrite:\n",
    "#     fwrite.writelines([line+'\\n' for line in extracted_words_hanzii_popularity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(thienchuu_set.difference(top_words_1m_characters))=941\n",
      "len(top_words_1m_characters.difference(thienchuu_set))=7706\n",
      "len(thienchuu_set.difference(top_words_100k_characters))=3335\n",
      "len(top_words_100k_characters.difference(thienchuu_set))=2274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'{len(thienchuu_set.difference(top_words_1m_characters))=}')\n",
    "print(f'{len(top_words_1m_characters.difference(thienchuu_set))=}')\n",
    "\n",
    "print(f'{len(thienchuu_set.difference(top_words_100k_characters))=}')\n",
    "print(f'{len(top_words_100k_characters.difference(thienchuu_set))=}')\n",
    "\n",
    "len(thienchuu_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words_to_add.txt', 'w', encoding='utf-8') as fwrite:\n",
    "    fwrite.writelines(sorted([word+'\\n' for word in words_to_add_set if hanzidentifier.is_simplified(word)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get new recommedations based on popularity of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out new recommendations\n"
     ]
    }
   ],
   "source": [
    "from tools_configs import *\n",
    "from pinyin import get as get_pinyin\n",
    "from Levenshtein import distance\n",
    "\n",
    "MAX_LEVENSHTEINDIST = 2\n",
    "MAX_RECOMMENDATIONS = 20\n",
    "\n",
    "recommended_words_dict = {}\n",
    "\n",
    "for headword in extracted_data:\n",
    "    recommended_words = []\n",
    "\n",
    "    data = extracted_data[headword]\n",
    "\n",
    "    recommendations = data['recommedations']\n",
    "\n",
    "    rec_list = []\n",
    "    rec_dict = {}\n",
    "\n",
    "    for rec in recommendations:\n",
    "        word = rec['chinese']\n",
    "        word = re.sub(PATTERN_REDUNDANT, \"\", word)\n",
    "        rec_dict[word] = {'mean': rec['mean'], 'pinyin': rec['pinyin']} \n",
    "        rec_list.append(word)\n",
    "\n",
    "    word_ranks = {}\n",
    "    new_recc_word = []\n",
    "\n",
    "    for word in list_hsk_1_4_list+rec_list:\n",
    "        if word == headword or word not in top_words_50k_index:\n",
    "            continue\n",
    "\n",
    "        # if len(word) + len(headword) > 3 and levenshtein(word, headword) < MAX_LEVENSHTEINDIST:\n",
    "        #     print(f'{word} vs {headword}')\n",
    "\n",
    "        if (word.find(headword) > -1 or (len(word) + len(headword) > 3 and distance(word, headword) < MAX_LEVENSHTEINDIST)):\n",
    "            word_ranks[word] = top_words_50k_index[word]\n",
    "\n",
    "    # hsk_word_ranks.sort(headword = lambda x: x[1])\n",
    "\n",
    "    # for rec in recommendations:\n",
    "    #     rec_word = rec['chinese']\n",
    "    #     if rec_word != headword and rec_word.find(headword) > -1 and rec_word in top_words_50k_index:\n",
    "    #         word_ranks[rec_word] = top_words_50k_index[rec_word]\n",
    "\n",
    "    for rec in recommendations:\n",
    "        if (word := rec['chinese']) not in word_ranks and word != headword:\n",
    "            word_ranks[word] = 2000000\n",
    "\n",
    "    sorted_reccoms = sorted(list(word_ranks.items()), key = lambda x: x[1])\n",
    "\n",
    "    added = 0\n",
    "    for rec in sorted_reccoms:\n",
    "        word =  rec[0]\n",
    "        if word == headword:\n",
    "            continue\n",
    "        \n",
    "        added += 1\n",
    "        if added >= MAX_RECOMMENDATIONS:\n",
    "            break\n",
    "\n",
    "        if word in rec_dict:\n",
    "            recommended_words.append({word: {'mean': rec_dict[word]['mean'], 'pinyin': rec_dict[word]['pinyin'], 'rank': rec[1]}})\n",
    "        elif word in extracted_data:\n",
    "            new_data = extracted_data[word]\n",
    "            if (list_items := list(new_data['wordkinds']['list_items'].items())):\n",
    "                mean = list_items[0][1][0]['definition']['vietnamese']\n",
    "                pinyin = get_pinyin(word)\n",
    "                recommended_words.append({word: {'mean': mean, 'pinyin': pinyin, 'rank': rec[1]}})\n",
    "\n",
    "    recommended_words_dict[headword] = recommended_words[:20]\n",
    "\n",
    "print('Writing out new recommendations')\n",
    "with open('new_reccommendations.json', 'w', encoding='utf-8') as fwrite:\n",
    "    json.dump(recommended_words_dict, fwrite, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open json file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print('Open json file')\n",
    "with open(\"new_reccommendations.json\", \"r\", encoding=\"utf-8\") as fread:\n",
    "    new_recomend = json.load(fread)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "㗎\n",
      "一\n",
      "一定 cần phải; chắc chắn (biểu thị kiên quyết hoặc xác định) yīdìng\n",
      "一起 cùng nơi; cùng một chỗ yīqǐ\n",
      "一直 thẳng; thẳng tuốt; cứ yīzhí\n",
      "一样 như nhau; giống nhau; cũng như; cũng thế yīyáng\n",
      "一下 một tý; một lúc; thử xem; một cái yīxià\n",
      "一般 giống nhau; như nhau yībān\n",
      "一切 tất cả; hết thảy yīqiē\n",
      "统一 thống nhất tǒngyī\n",
      "一边 mặt bên; một mặt yībiān\n",
      "一旦 một ngày; chốc lát yīdàn\n",
      "一致 nhất trí; không chia rẽ yīzhì\n",
      "一辈子 cả đời; một đời; suốt đời; 骂语【罵語】 yībèizǐ\n",
      "一度 một lần; một trận yīdù\n",
      "一会儿 một chốc; một lát; lát; nả; một chặp; chặp yīhùiér\n",
      "一向 gần đây; thời gian qua yīxiàng\n",
      "一流 cùng loại yīlíu\n",
      "一共 tổng cộng; gồm; hết thảy yīgòng\n",
      "一再 nhiều lần; năm lần bảy lượt yīzài\n",
      "一贯 nhất quán; trước sau như một (tư tưởng, tác phong) yīguàn\n"
     ]
    }
   ],
   "source": [
    "headwords = list(new_recomend.keys())\n",
    "\n",
    "for headword in headwords[:2]:\n",
    "    print(headword)\n",
    "    reccs = new_recomend[headword]\n",
    "\n",
    "    for rec in reccs:\n",
    "        key = list(rec.keys())[0]\n",
    "        \n",
    "        item = rec[key]\n",
    "\n",
    "        print(f\"{key} {item['mean']} {item['pinyin']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
